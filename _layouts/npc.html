{% include base_path %}

<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>NPC: Neural Point Characters</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://lemonatsu.github.io/{{ base_path }}/files/npc/img/rays_square.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://lemonatsu.github.io/npc/"/>
    <meta property="og:title" content="NPC: Neural Point Characters" />
    <meta property="og:description" content="Project page for NPC: Neural Point Characters from Video." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="npc" />
    <meta name="twitter:description" content="Project page for NPC: Neural Point Characters from Video." />
    <meta name="twitter:image" content="https://lemonatsu.github.io/{{ base_path }}/files/npc/img/rays_square.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="{{ base_path }}/files/npc/img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="{{ base_path }}/assets/css/npc/app.css">

    <link rel="stylesheet" href="{{ base_path }}/assets/css/npc/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="{{ base_path }}/assets/js/npc/js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>NPC</b>: Neural Point Characters from Video </br>
                <small>
                    arXiv 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://lemonatsu.github.io/">
                            Shih-Yang Su
                        </a>
                        </br>University of British Columbia
                    </li>
                    <li>
                        <a href="https://scholar.google.ch/citations?user=oLi7xJ0AAAAJ&hl=en">
                            Timur Bagautdinov
                        </a>
                        </br>Reality Labs Research
                    </li>
                    <li>
                        <a href="https://www.cs.ubc.ca/~rhodin/web/">
                            Helge Rhodin
                        </a>
                        </br>University of British Columbia
                    </li><br>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="{{ base_path }}/files/npc/img/paper_img.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/0uzO6KL6BEY">
                            <image src="{{ base_path }}/files/npc/img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="{{ base_path }}/files/npc/img/github.png" height="60px">
                                <h4><strong>Code </br>(coming soon)</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="{{ base_path }}/files/npc/img/teaser_blur.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    High-fidelity human 3D models can now be learned directly from videos, typically by combining a template-based surface model with neural representations. 
                    However, obtaining a template surface requires expensive multi-view capture systems, laser scans, or strictly controlled conditions. 
                    Previous methods avoid using a template but rely on a costly or ill-posed mapping from observation to canonical space. 
                    We propose a hybrid point-based representation for reconstructing animatable characters that does not require an explicit surface model, while being generalizable to novel poses. 
                    For a given video, our method automatically produces an explicit set of 3D points representing approximate canonical geometry, and learns an articulated deformation model that produces pose-dependent point transformations. 
                    The points serve both as a scaffold for high-frequency neural features and an anchor for efficiently mapping between observation and canonical space. 
                    We demonstrate on established benchmarks that our representation overcomes limitations of prior work operating in either canonical or in observation space. 
                    Moreover, our automatic point extraction approach enables learning models of human and animal characters alike, matching the performance of the methods using rigged surface templates despite being more general.
                </p>
                <br>
                <b>We blur all faces for anonymity.</b>
                </br>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video 
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/0uzO6KL6BEY" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <image src="{{ base_path }}/files/npc/img/overview_blur.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    NPC produces a volume rendering of a character with a NeRF Fψ locally conditioned on features
                    aggregated from a dynamically deformed point cloud. Given a raw video, we first estimate a canonical point cloud p with an
                    implicit body model. GNN then deforms canonical points p conditioned on skeleton pose θ, and produces a set
                    of pose-dependent per-point features. Every 3D query point qo in the observation space aggregates
                    the features from k-nearest neighbors in the posed point cloud. The aggregated feature is passed into Fψ for the volume
                    rendering. Our model is supervised directly with input videos.
                </p>
                <br>
                </br>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Point Feature Encoding
                </h3>
                <image src="{{ base_path }}/files/npc/img/point_feature_encoding.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Our core idea is to employ a point cloud p as an anchor to carry features from the
                    canonical to the observation space, forming an efficient mapping between the two. (1) Each p carries a learnable feature fp
                    and its position queries features fs from a canonical field. (2) The GNN adds pose-dependent features fθ and deformation
                    Δp. (3) The view direction and distance is added in bone-relative space. (4) The k-nearest neighbors of
                </p>
                <br>
                </br>
            </div>
        </div>
        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Integrated Positional Encoding
                </h3>
                <p class="text-justify">
                    Typical positional encoding (as used in Transformer networks and Neural Radiance Fields) maps a single point in space to a feature vector, where each element is generated by a sinusoid with an exponentially increasing frequency:
                </p>
                <p style="text-align:center;">
                    <image src="{{ base_path }}/files/npc/img/pe_seq_eqn_pad.png" height="50px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="{{ base_path }}/files/npc/img/pe_anim_horiz.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    Here, we show how these feature vectors change as a function of a point moving in 1D space.
                    <br><br>
                    Our <em>integrated positional encoding</em> considers Gaussian <em>regions</em> of space, rather than infinitesimal points. This provides a natural way to input a "region" of space as query to a coordinate-based neural network, allowing the network to reason about sampling and aliasing. The expected value of each positional encoding component has a simple closed form:
                </p>
                <p style="text-align:center;">
                    <image src="{{ base_path }}/files/npc/img/ipe_eqn_under_pad.png" height="30px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="{{ base_path }}/files/npc/img/ipe_anim_horiz.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    We can see that when considering a wider region, the higher frequency features automatically shrink toward zero, providing the network with lower-frequency inputs. As the region narrows, these features converge to the original positional encoding.
                </p>
            </div>
        </div>
        -->


        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We train NeRF and mip-NeRF on a dataset with images at four different resolutions. Normal NeRF (left) is not capable of learning to represent the same scene at multiple levels of detail, with blurring in close-up shots and aliasing in low resolution views, while mip-NeRF (right) both preserves sharp details in close-ups and correctly renders the zoomed-out images.
                </p>                
                <br>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="{{ base_path }}/files/npc/img/ship_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="{{ base_path }}/files/npc/img/chair_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="{{ base_path }}/files/npc/img/lego_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="{{ base_path }}/files/npc/img/mic_sbs_path1.mp4" type="video/mp4" />
                </video>
                <br><br>
                <p class="text-justify">
                    We can also manipulate the integrated positional encoding by using a larger or smaller radius than the true pixel footprint, exposing the continuous level of detail learned within a single network:
                </p>     
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="{{ base_path }}/files/npc/img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />
                </video>
            </div>
        </div>
        -->


        <!--
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    Our implementation is based on <a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-PyTorch</a>.
                </p>
            </div>
        </div>
        -->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{su2023npc,
    title={NPC: Neural Point Characters from Video},
    author={Su, Shih-Yang and Bagautdinov, Timur and Rhodin, Helge},
    journal={arXiv},
    year={2023}
}
                </textarea>

                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Datasets
                </h3>

                All data sourcing, modeling codes, and experiments were developed at University of British Columbia. Meta did not obtain the data/codes or conduct any experiments in this work.
                <h4>
                    Human3.6M
                </h4>
                <div class="form-group col-md-20 col-md-offset-1">
                    <textarea id="bibtex2" class="form-control" readonly>
@article{h36m_pami,
    author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu,  Cristian},
    title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    publisher = {IEEE Computer Society},
    volume = {36},
    number = {7},
    pages = {1325-1339},
    month = {jul},
    year = {2014}
}

@inproceedings{IonescuSminchisescu11,
    author = {Catalin Ionescu, Fuxin Li, Cristian Sminchisescu},
    title = {Latent Structured Models for Human Pose Estimation},
    booktitle = {International Conference on Computer Vision},
    year = {2011}
}
                </textarea>

                </div>
                <h4>
                    MonoPerfCap
                </h4>
                <div class="form-group col-md-20 col-md-offset-1">
                    <textarea id="bibtex3" class="form-control" readonly>
@article{xu18monoperfcap,
    author = {W. Xu and A. Chatterjee and M. Zollh{\"o}fer and H. Rhodin and D. Mehta and H.-P. Seidel and C. Theobalt},
    title = {Monoperfcap: Human Performance Capture from Monocular Video},
    journal = TOG,
    volume = "37",
    number = "2",
    pages = "27",
    year = 2018
}
                </textarea>



                </div>
                <h4>
                    AIST++
                </h4>
                <div class="form-group col-md-20 col-md-offset-1">
                    <textarea id="bibtex4" class="form-control" readonly>
@inproceedings{li2021learn,
    title={AI Choreographer: Music Conditioned 3D Dance Generation with AIST++}, 
    author={Li, Ruilong and Yang, Shan and Ross, David A. and Kanazawa, Angjoo},
    year={2021},
    booktitle=ICCV,
}

                </textarea>
                </div>
                <h4>
                    SURREAL
                </h4>
                <div class="form-group col-md-20 col-md-offset-1">
                    <textarea id="bibtex5" class="form-control" readonly>
@inproceedings{varol17surreal,  
    title     = {Learning from Synthetic Humans},  
    author    = {Varol, G{\"u}l and Romero, Javier and Martin, Xavier and Mahmood, Naureen and Black, Michael J. and Laptev, Ivan and Schmid, Cordelia},  
    booktitle = {CVPR},  
    year      = {2017}  
  }
                </textarea>

                </div>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Shaofei Wang and Ruilong Li for helpful discussions related to ARAH and TAVA. 
                We thank Luis A. Bolaños for his help and discussions, and Frank Yu, Chunjin Song, Xingzhe He and Eric Hedlin for their insightful feedback. 
                We also thank <a href="https://arc.ubc.ca/">Advanced Research Computing at the University of British Columbia</a> and <a href="https://docs.alliancecan.ca/wiki/Getting_started">Compute Canada</a> for providing computational resources.
                </br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>